# Binaural Acoustic Drone Detection and Direction of Arrival Estimation
In this project I am studying the human auditory system and trying to rebuild its architecture
in a simplified model. I applied this model to direction of arrival estimation of dronesounds.
The beauty of the human auditory system lies in its genius signal processing. In the context of DOA estimation 
modern algorithms are noise sensitive, usually require an array of microphones, while instead our brain can tell 
the sound source with 2 "microphones" or ears only. It is thought that the brain can tell this through a fusion of 
Interaural Time Difference Measurement, Interaural Level Measurement and the observation of Spatial Sound Cues that get introduced 
into the signal through the shape of the auricle.

## Algorithm Description
The human auditory system is very complex and not fully understood, here I am trying to approximate parts of it.
The cochlea is modeled through a filter bank, which divides the incoming sound into 800 frequency channels.
The firing of neurons at sound oscillation peaks is modeled by highlighting peaks in a time frequency space.
2 different models have been tried for direction estimation from this input:
## Model A 
Model A is a Convolutional Neural Network with 2x2 feature maps as inputs. One branch of the CNN compares a feature map derived from the left and right input
that highlights interaural time differences, the second branch compares a feature map derived from the left and right input, that highlights interaural level differences
and interaural cues. Later the branches merge.
Output of the model are 30 neurons that indicate the directions (0-360 degrees, 12 degree steps) of the ariving sound.
| Signal Peaks Left | Signal Cues Left|
<img src="images/bw.png" alt="Signal Peaks" width="400" height="200" style="object-fit: cover;">  <img src="images/cues.png" alt="Signal Cues" width="400" height="200" style="object-fit: cover;"> 

## Model B
Model A is a Convolutional Neural Network with 3 feature maps as inputs. Two feature maps highlight the interaural cues and level differences between left and right ear, 
the 3rd feature map is a correlogram highlighting ITD between left and right input.
Output of the model are 30 neurons that indicate 30 directions (0-360 degrees, 12 degree steps) of the ariving sound.
| Correlogram Left and Right | Signal Cues Left|

<img src="images/correlogram.png" alt="Signal Peaks" width="400" height="200" style="object-fit: cover;">  <img src="images/cues.png" alt="Signal Cues" width="400" height="200" style="object-fit: cover;"> 
## Experiment
Both models were trained on synthetically generated sound received from a KEMAR mannequin in a simulated room environment from a single sound source,
as well as tested on real world data generated by a M-Track Duo Audio Processor and two connected microphones that where placed inside a clay head.
Both models converge for all environments. Tests on live generated data are ongoing.
## Explanation of Scripts
The scripts are organized in a step-by-step way.
Step_1 offers the option to either generate synthetic data (Step_1_synth_data_gen), or to record experimental data (Step_1_experimental_data_recording)

Step_2 is a preprocessing step depending on the model to be used, choose between model_A and model_B

Step_4 splits the dataset into training and validation data.

Step_5 is the training step, where the models are trained.

Step_6 offers intereference of the models with a soundfile.

Step_7 offers live interference with a soundstream recorded by the M-Track Duo audioprocessor.

![Alt text describing the image](image.jpg)